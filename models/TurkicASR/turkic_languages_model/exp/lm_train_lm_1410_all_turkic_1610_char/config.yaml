config: conf/train_lm_1410.yaml
print_config: false
log_level: INFO
dry_run: false
iterator_type: sequence
output_dir: exp/lm_train_lm_1410_all_turkic_1610_char
ngpu: 1
seed: 0
num_workers: 1
num_att_plot: 3
dist_backend: nccl
dist_init_method: env://
dist_world_size: null
dist_rank: null
local_rank: 0
dist_master_addr: null
dist_master_port: null
dist_launcher: null
multiprocessing_distributed: false
unused_parameters: false
sharded_ddp: false
cudnn_enabled: true
cudnn_benchmark: false
cudnn_deterministic: true
collect_stats: false
write_collected_feats: false
max_epoch: 50
patience: null
val_scheduler_criterion:
- valid
- loss
early_stopping_criterion:
- valid
- loss
- min
best_model_criterion:
-   - valid
    - loss
    - min
keep_nbest_models: 10
nbest_averaging_interval: 0
grad_clip: 5.0
grad_clip_type: 2.0
grad_noise: false
accum_grad: 1
no_forward_run: false
resume: true
train_dtype: float32
use_amp: false
log_interval: null
use_matplotlib: true
use_tensorboard: true
create_graph_in_tensorboard: false
use_wandb: false
wandb_project: null
wandb_id: null
wandb_entity: null
wandb_name: null
wandb_model_log_interval: -1
detect_anomaly: false
pretrain_path: null
init_param: []
ignore_init_mismatch: false
freeze_param: []
num_iters_per_epoch: null
batch_size: 20
valid_batch_size: null
batch_bins: 4000000
valid_batch_bins: null
train_shape_file:
- exp/lm_stats_all_turkic_1610_char/train/text_shape.char
valid_shape_file:
- exp/lm_stats_all_turkic_1610_char/valid/text_shape.char
batch_type: numel
valid_batch_type: null
fold_length:
- 150
sort_in_batch: descending
sort_batch: descending
multiple_iterator: false
chunk_length: 500
chunk_shift_ratio: 0.5
num_cache_chunks: 1024
train_data_path_and_name_and_type:
-   - dump/raw/lm_train_all_turkic_1610_sp.txt
    - text
    - text
valid_data_path_and_name_and_type:
-   - dump/raw/dev_all_turkic_1610/text
    - text
    - text
allow_variable_data_keys: false
max_cache_size: 0.0
max_cache_fd: 32
valid_max_cache_size: null
optim: adam
optim_conf:
    lr: 0.001
scheduler: warmuplr
scheduler_conf:
    warmup_steps: 25000
token_list:
- <blank>
- <unk>
- <space>
- a
- а
- i
- е
- ы
- н
- n
- e
- r
- т
- р
- l
- л
- o
- d
- k
- д
- m
- і
- s
- t
- y
- с
- м
- к
- ә
- u
- b
- б
- ı
- о
- и
- у
- h
- й
- g
- қ
- п
- ш
- ғ
- z
- ң
- ө
- г
- '[tr]'
- з
- ү
- ж
- ü
- ş
- '[ba]'
- v
- ҡ
- '[uz]'
- c
- q
- '[kk]'
- ҙ
- ى
- ç
- p
- һ
- ğ
- ұ
- ö
- я
- ا
- f
- х
- ە
- ن
- ل
- в
- ر
- э
- ت
- x
- ۇ
- ҫ
- j
- ئ
- ق
- م
- د
- ч
- ك
- ي
- ф
- ب
- س
- پ
- ю
- و
- '[tt]'
- '[ug]'
- ش
- ӑ
- ې
- ӗ
- ڭ
- غ
- ز
- ۈ
- ь
- '[ky]'
- چ
- ц
- گ
- ۆ
- ʻ
- ھ
- '[cv]'
- ۋ
- خ
- ج
- җ
- ъ
- w
- ʼ
- '[sah]'
- ҕ
- ӳ
- ҥ
- щ
- â
- ف
- ё
- ə
- ژ
- '[az]'
- õ
- û
- ӊ
- ҳ
- î
- ў
- ʙ
- ó
- ģ
- ò
- ӯ
- ë
- ј
- ƣ
- µ
- ƶ
- <sos/eos>
init: null
model_conf:
    ignore_id: 0
use_preprocessor: true
token_type: char
bpemodel: null
non_linguistic_symbols: data/train_all_turkic_1610/nonlyng.txt
cleaner: null
g2p: null
lm: transformer
lm_conf:
    pos_enc: null
    embed_unit: 128
    att_unit: 512
    head: 8
    unit: 2048
    layer: 16
    dropout_rate: 0.1
required:
- output_dir
- token_list
version: '202207'
distributed: false
